---
title: 2024.07.03
description: 기본 Pytorch를 공부합니다.
categories: [TIL, Machine Learning]
tags: [til, machine learning, deep learning, pytorch]
date: 2024-07-03 22:40:00 +0900
math: True
---

<hr>
<h3> PyTorch Basics </h3>

pytorch: Dynamic Computation Graph (실행을 하며 그래프 생성)

* PyTorch 의 특징 <br/>
: Numpy 구조를 가지는 Tensor 객체로 array 표현 <br/>
: 자동미분(<span style="color:red;">AutoGrad</span>)을 지원해 DL 연산 지원 <br/>
: 다양한 형태의 DL (Dataset, Multi-GPU) 을 지원하는 함수와 모델을 지원 <br/>

* Tensor 
: 다차원 Arrays 를 표현하는 PyTorch class

* numpy like operations

```python
import torch

data [[3, 5, 20],[10, 5, 50],[1, 5, 10]]
x_data = torch.tensor(data)

x_data[1:]
x_data[:2, 1:]
x_data.flatten()
torch.ones_like(x_data)
x_data.numpy()
x_data.shape
x_data.dtype
```
{: .nolineno }

< Tensor handling > <br/>
1. view: reshape와 동일하게 tensor의 shape 변환 <br/>
2. squeeze: 차원의 개수가 1인 차원 삭제 <br/>
3. unsqueeze: 차원의 개수가 1인 차원 추가 <br/>


* 행렬곱셈 연산은 함수 dot (x), mm 사용함 <br/>
벡터 간 내적을 구할 때, dot 쓰면 되지만 <br/>
행렬 간 연산시, mm 사용 (t1.mm(t2)) <br/>

```python
a = torch.rand(5, 2, 3) # 5 batch, 2 by 3
b = torch.rand(3) # 3 by 1
a.mm(b) # 연산 안됨
a.matmul(b) # 연산 됨 <- broadcasting 지원
```
{: .nolineno }

* Tensor operations for ML/DL formula <br/>
=> nn.functional 모듈 통해 수식 변환 지원

```python
import torch
import torch.nn.functional as F

tensor = torch.FloatTensor([0.5, 0.7, 0.1])
h_tensor = F.softmax(tensor, dim=0)
h_tensor
# tensor([0.3458, 0.4224, 0.2318])

y = torch.randint(5, (10,5))
y_label = y.argmax(dim=1)

torch.nn.functional.one_hot(y_label) # 자동으로 원핫 인코딩
```
{: .nolineno }

* torch autograd <br/>
=> backward 함수 사용 <br/>

$$
y = w^2
$$

$$
z = 10*y + 25
$$

$$
z = 10*w^{2} + 25
$$

```python
w = torch.tensor(2.0, requires_grad=True)
y = ww**2
z = 10*y + 25
z.backward()
w.grad
```
{: .nolineno }

