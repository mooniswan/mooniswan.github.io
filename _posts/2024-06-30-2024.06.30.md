---
title: 2024.06.30
description: 머신러닝을 다루기 위한 기초수학을 공부합니다.
categories: [TIL, Machine Learning]
tags: [til, machine learning, mathematics]
date: 2024-06-30 13:00:00 +0900
---

<h2> 벡터 </h2>

: 숫자를 원소로 가지는 리스트 or 배열 <br/>
== 원점에서의 상대적 위치 <br/>

벡터의 노름: 원점에서의 거리 <br/>
$L_{1} norm$: 각 성분의 변화량 절대값의 합 <br/>
$L_{2} norm$: 유클리드 거리(성분곱의 합에 제곱근 연산) <br/>
=> 기하학적 성질에 따라 두가지 모두 사용 <br/>
<span style="color:pink;"> 
L1 norm 상의 원 => Robust 학습, Lasso 회귀 <br/>
L2 norm 상의 원 => Laplace 근사, Ridge 회귀
</span>

```python
def l1_norm(x):
    x_norm = np.abs(x) #절댓값
    x_norm = np.sum(x_norm)
    return x_norm

def l2_norm(x):
    x_norm = x*x
    x_norm = np.sum(x_norm)
    x_norm = np.sqrt(x_norm) #제곱근
    return x_norm
# 또는 np.linalg.norm(x)
``` 

* <b> 두 벡터 사이의 <span style="color: red;"> 거리</span></b> => 유클리드 거리 사용 <br/>
즉, 벡터의 뺄셈 이용 (순서 바뀌어도 결과는 동일) <br/>
1. Numpy 라이브러리 이용
```python
dist = np.linalg.norm(a - b) #L2 norm
```
2. scipy 라이브러리의 distance 모듈 이용
```python
dist = distance.euclidean(a, b)
```

* <b> 두 벡터 사이의<span style="color: red;"> 각도</span></b> 
=> L2 norm만 가능
```python
# 라디안 단위 각도 계산
def angle(x, y):
    v = np.inner(x, y) / (l2_norm(x)*l2_norm(y)) # 분자:inner product(내적)
    theta = np.arccos(v) #코사인의 역함수, 즉, 주어진 코사인 값에 해당하는 각 반환
    return theta
```

* 내적(inner product) <br/>
: 정사영(orthogonal projection)의 길이를 벡터 y의 길이만큼 조정한 값 == ||y|| Proj(x)<br/>
<span style="color:pink;"> => 벡터 간의 유사도 측정시 사용</span> <br/>
Proj(x) == ||x||cosθ <br/>

즉, 벡터 x와 y의 내적: x의 노름 x y의 노름 x 두 벡터 사이의 각도
```python
dot_product = np.dot(a, b)
```


<h2> 행렬 </h2>

1. 벡터를 원소로 가지는 2차원 배열(n x m 행렬)
2. 벡터공간에서 사용되는 연산자
: 행렬곱을 통해 벡터(점)를 다른 차원의 공간으로 보냄 <br/>
=> 패턴 추출 or 데이터 압축
$x_{i}$ i번째 데이터
$x_{ij}$ i번째 데이터의 j번째 변수의 값 <br/>

* 전치행렬
: 행과 열의 인덱스가 바뀐 행렬 <br/>

> 행렬의 덧셈, 뺼셈, 성분곱, 스칼라곱은 벡터 계산방법과 동일

* 행렬 곱셈(X@Y) <br/>
: i번째 행벡터와 j번째 열벡터 사이의 내적을 성분으로 가지는 행렬 계산

np.inner은 i번째 행벡터와 j번째 <span style="color:red;">행</span>벡터 사이의 내적을 성분으로 가지는 행렬을 계산함. <br/>

* 역행렬 (inverse matrix)
: 행과 열 숫자가 같고,  행렬식(determinant)이 0이 아닌 경우에만 가능 <br/>
```python
np.linalg.inv(X) #inverse를 의미
```

위 조건이 성립되지 않을 경우, <br/>
<<<<<<< HEAD
유사역행렬(pseudo-inverse) or 무어-펜로즈(Moore-Penrose) 역행렬 사용: $A^+$

n >= m 인 경우 <br/>
$A^+ = (A^TA)^{-1}A^T$ <br/>
=> ($A^+A = I$ 성립) <br/>
n <= m 인 경우 <br/>
$A^+ = A^T(AA^T)^{-1}$ <br/>
=> ($AA^+ = I$ 성립) <br/>
=======
유사역행렬(pseudo-inverse) or 무어-펜로즈(Moore-Penrose) 역행렬 사용
>>>>>>> refs/remotes/origin/master

```python
np.linalg.pinv(Y) #pseudo inverse
```

<선형회귀분석> <br/>
: 무어 펜로즈 역행렬을 사용해 데이터를 선형모델로 해석하는 선형회귀식(L2 norm을 최소화하는 β) 찾을 수 있음 <br/>
n >= m인 경우: 데이터가 변수 개수보다 많거나 같아야 함<br/>

```python
#1. Scikit Learn 을 활용한 회귀분석
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)
y_test = model.predict(x_test)
#2. Moore-Penrose 역행렬
X_ = np.array([np.append(x,[1]) for x in X]) # intercept 항 추가
beta = np.linalg.pinv(X_) @ y
y_test = np.append(x, [1]) @ beta
```
<br/>

> 네이버 boostcourse의 인공지능 기초 다지기 강의를 수강하며 정리한 내용입니다.