---
title: 2024.07.04
description: Basic AI 지식을 공부합니다.
categories: [TIL, Machine Learning]
tags: [til, ai, machine learning]
date: 2024-07-04 23:40:00 +0900
math: True
---

<hr>
* 벡터의 노름
: 벡터의 크기 or 길이를 나타내는 값으로, 원점에서부터의 거리를 의미한다 <br/>

머신러닝에서는 L1, L2 노름의 성질을 모두 필요로 하며 <br/>
1. L1 노름 -> 희소성 강조 <br/>
2. L2 노름 -> 전체적인 크기를 평가 <br/>
할 때 사용된다 <br/>

* L1 노름 <br/>
: 벡터의 각 성분의 절댓값을 모두 더한 값으로, 성분의 변화량을 평가할 때 사용한다 <br/>
$$ x=[0,1,2], L_1 norm = 3 $$
* L2 노름 <br/>
: 피타고라스 정리를 활용한 `유클리드 거리`를 계산한 값이며, 주로 벡터 간 직선거리를 측정할 때 사용한다 <br/>
$$ x=[-6, -8], L_2 norm = 10 $$

- 벡터의 뺄셈을 이용해 두 벡터 사이의 거리를 계산할 수 있다 <br/>
- 두 노름의 선택에 따라 벡터 사이의 거리가 다를 수 있다 
<br/>
<br/>

* <b> 내적 </b> 

$$ 
< x, y > = \sum_{i=1}^{n} x_i y_i
$$

<span style="color:green;"> Q)  두 벡터 (x, y)의 내적은? <br/> </span>
$$ x = [1, -1, 1, -1] $$ , 
$$ y = [4, -4, 4, -4] $$ <br/>
A) $$ 4 + 4 + 4 + 4 $$ <br/>
=<span style="color:green;"> 16 </span> 

* 행렬 A의 역행렬을 구하기 위해서는, $$ A * A^{-1} = 항등행렬 I $$ 가 되는 걸 찾으면 된다
<br/>
<br/>

* 표본 분산
: 각 데이터 값과 평균의 차이를 제곱해 합산 후, 제곱합을 '데이터 수 - 1' 로 나눈 값 <br/>
$$ X = {1,2,3,4,5} $$ <br/>
1. $$ mean: 3 $$ <br/>
2. $$ 4 + 1 + 0 + 1 + 4 = 10 $$ <br/>
3. $$ 10/4 = 2.5 $$ <br/>

<hr>
* <b> 베이즈 통계학 </b>
1. 인과관계를 알아내기 위해서는 중첩요인의 효과를 제거하고 원인에 해당하는 변사만의 인관관계를 계산해야 한다
2. 인과관계는 데이터 분포의 변화에 강건한 예측모형을 만들 때 필요하다
3. 중첩요인의 효과를 제거하지 않으면 가짜연관성이 나온다
4. 인과관계만으로는 높은 예측 정확도를 담보하기 어렵다
5. 데이터가 많아질경우 조건부 확률을 가지고 함부로 인과관계를 추론할 수 `없음`
<br/>
<br/>

<span style="color:green;"> Q1) 어떤 질병의 발병률이 10%이다. 이때, 이 질병에 실제로 걸렸을 때 검진 시 양성으로 뜰 확률은 99%, 걸리지 않았을 때 양성으로 뜰 확률이 5%이다. 어떤 사람이 질병 검사 후 양성 판정을 받았을 때, 그 사람이 실제로 질병에 감염되었을 확률을 소수 첫 번째 자리에서 반올림한 결과는 무엇일까? </span> <br/>
A) [정보]
발병률 $$ P(D): 0.1, P(T|D): 0.99, P(T|\neg D): 0.05 $$ <br/>
[구해야 하는 결과] $$ P(D|T) $$ <br/>

$$ 
P(D | T) = \frac{P(T | D) \cdot P(D)}{P(T)} 
$$

[전체가 양성 판정일 확률] <br/>

$$ 
P(T) = P(T | D) \cdot P(D) + P(T | \neg D) \cdot P(\neg D)
 $$

즉, $$ P(T) = 0.99 * 0.1 + 0.05 * 0.9 $$ <br/>
$$ = 0.099 + 0.045 = 0.144 $$ <br/>

[최종 결과] <br/>
$$ P(D|T) = \frac{0.99 * 0.1}{0.144} $$
$$ = \frac{99}{144} $$ <br/>
= <span style="color:green;"> $$ \frac{11}{16} $$  </span>
<br/>
<br/>

<span style="color:green;"> Q2) 한 학급에 학생이 100명이 있다. 여학생 30% 중 3%가 외국인이다. 또 남학생 70% 중 8%가 외국인이다. 해당 학급에서 임의로 뽑은 1명이 외국인일 때, 이 학생이 여학생일 확률을 구하시오(소수점 셋째자리에서 반올림) </span> 
<br/>
A) [베이즈정리 사용]

$$ 
P(A|B) = \frac{P(B | A) * P(A)}{P(B)} 
$$ 

<br/>
A: 여학생, B: 외국인 <br/>
외국인일 확률 $$ P(B) = P(|) * P() + P() * P(|) $$ <br/>
$$ = 0.03 * 0.3 + 0.08 * 0.7 = 0.009 + 0.056 = 0.065 $$ <br/>

즉, 구하고자 하는 (A|B) 는 <br/>
$$ = \frac{0.03 * 0.3}{0.065} = \frac{9}{65} $$
<br/>
= <span style="color:green;"> 0.14 </span>
<br/>
<br/>

<span style="color:green;"> Q3) 두 개의 노트북 조립라인을 가진 공장에서 생산된 1000대 씩의 노트북들을 같은 화물 창고에 쌓아 놓았다. 각각의 조립라인을 정밀하게 조사하여, 1번 조립라인에서 생산된 노트북의 10%가 불량이고, 2번 조립라인에서 생산된 노트북의 15%가 불량임을 알게 되었다. 화물 창고의 노트북을 하나 꺼내어 조사한 결과 불량이었을 때, 이 노트북이 1번 조립라인에서 생산되었을 확률을 구하시오. <br/> </span> 
A) [구해야하는 답] 

$$ 
P(A | B) = \frac{P(B | A) * P(A)}{P(B)} 
$$ 

<br/>
A = 1번일 확률, B = 불량일 확률 <br/>

$$ P(A) = 0.5, P(B | A) = 0.1 $$ <br/>
P(B) = 1번일때불량 x 1번 + 2번일때불량 x 2번 <br/>
$$ = 0.1 * 0.5 + 0.15 * 0.5 = 0.05 + 0.075 = 0.125 $$ <br/>
즉, 최종적으로 $$ P(A | B) = \frac{0.1 * 0.5}{0.125} = 2/5 $$ <br/>
= <span style="color:green;"> 0.4 </span>
<br/>
<hr>

* <b> 확률적 경사하강법 </b>
1. `일부` 데이터를 사용해서 업데이트한다
2. 전체 데이터가 아닌 미니배치를 써서 업데이트 하므로 연산량이 $$ /frac{b}{n} $$ 으로 감소하여 연산자원을 더 효율적으로 활용할 수 있다(n은 자연수)
$$
<br/>
<br/>

* <b> 미니배치 연산 </b>
1. 경사하강법은 전체데이터를 가지고 목적식의 그레디언트 벡터를 계산한다
2. 확률적 경사하강법은 미니배치를 가지고 그레디언트 벡터를 계산한다
3. 매번 다른 미니배치를 사용하기 때문에 곡선의 모양이 바뀌게 된다
4. 확률적 경사하강법은 볼록이 아닌 목적식에서도 사용가능하므로 경사하강법보다 머신러닝 학습에 효율적이다 (확률적 경사하강법 > 경사하강법)
<br/>
<br/>

* <b> Convolution 연산 </b>
: 연속변수에 대한 함수 f,g 사이의 convolution 수식

$$
[f * g](x) = \int_{\mathbb{R}^d} f(z)g(x - z) \, dz
$$


<span style="color:green;"> 
Q) 벡터 x와 h가 다음과 같이 주어질 때,  y_1 값을 구하시오. </span> <br/>
A) (m은 h의 원소의 갯수, 인덱스는 1부터 시작) <br/>

$$ 
x = [1, 2, 3], h = [4, 5]
$$

$$
y_n = \sum_{k=1}^{m} x_{n+k-1} \cdot h_k 
$$

= $$ x_1*h_1 + x_2*h_2 $$ <br/>
= <span style="color:green;"> 14 </span>

<span style="color:green;"> Q) 입력 행렬 X와 커널 K가 다음과 같이 주어질 때, $$ Y_{1, 2} $$ 값을 구하시오. </span> <br/>
p=1 ~ m=2, q=1 ~ n=2 <br/>

$$
Y_{i,j} = K_{p,q} X_{i+p-1,j+q-1} 
$$

$$ 1. 1,1 => 1 * X(1,2) = 1 * 0 = 0 $$ <br/>
$$ 2. 1,2 => 2 * X(1,3) = 2 * 1 = 2 $$ <br/>
$$ 3. 2,1 => 3 * X(2,2) = 3 * 1 = 3 $$ <br/>
$$ 4. 2,2 => 4 * X(2,3) = 4 * 0 = 0 $$ <br/>
=> $$ 0 + 2 + 3 + 0 $$ <br/>
= <span style="color:green;"> 5 </span>

* $$ X = [X_1, X_2, ..., X_t] $$ 같은 시퀀스 데이터 X가 주어졌을 때, 베이즈 법칙에 따라 전개한 X에 대한 확률분포는? <br/>
=> 베이즈 법칙과 조건부확률의 연쇄법칙 사용되어 전체확률 P(X)는 각 시점의 조건부확률의 곱으로 전개됨

* 시퀀스 길이가 길어지는 경우, Vanila RNN에서 발생할 수 있는 기울기 소실문제를 해결하기 위해 `LSTM`_Long Short-Term Memory 사용함
<br/>
<br/>

* <b> 로그가능도의 특징 </b>
1. 로그가능도를 최적화하는 모수는 가능도를 최적화하는 MLE가 된다
2. 데이터의 숫자가 매우 커지면 컴퓨터의 정확도로는 가능도를 계산하는 것이 어렵다
3. 데이터가 독립인 경우, 로그를 사용하면 `가능도의 곱셈`을 `로그가능도의 덧셈`으로 바꿀 수 있다
4. 대개 손실함수 경우 음의 로그가능도를 최적화할 수 있다.
5. 로그가능도를 사용하면 연산량 즉, 계산복잡도를 O(n^2) 에서 O(n) 으로 줄여준다
<br/>
<br/>

* <b> 딥러닝 </b>
1. 활성함수의 예시로는 시그모이드 함수, 하이퍼볼릭 탄젠트, 렐루 함수가 존재한다
2. softmax 함수는 모델의 출력을 확률로 해석할 수 있게 변환해 주는 연산으로 분류 문제를 해결할 때 도움을 준다
3. 역전파 알고리즘은 연쇄 법칙을 기반으로한 자동 미분을 통해 계산된다
4. 딥러닝은 비선형 모델인 신경망을 사용하며, 신경망은 선형 모델에서 활성 함수를 합성한 구조로 이루어졌다
5. 신경망의 층이 너무 적으면 모델이 복잡한 패턴을 학습하지 못할 수 있고, 너무 많으면 overfitting이나 학습이 어려워질 수 있다 따라서, 효율적인 학습은 층의 수, 데이터, 하이퍼파라미터 설정, 모델 아키텍처 등에 따라 달라진다 
<br/>
<br/>

$$ * ReLU(x) = max(0, x) $$ <br/>
: ReLU 함수는 입력값 x가 0보다 크면 그대로 반환, <br/>
0보다 작거나 같으면 0을 반환한다 <br/>

<span style="color:green;"> 
Q) $$ z = (k + 3)^3, k = (x + y)^2 $$ 와 같을 때 
$$ \frac {​∂z}{​∂x} $$ 의 값은? </span>
<br/>

$$ 
\frac{\partial z}{\partial x} = \frac{\partial z}{\partial k} \cdot \frac{\partial k}{\partial x} 
$$

즉, z를 k에 대해 미분한 값, k를 x에 대해 미분한 값을 진행해준다. <br/>
1. $$ 3(k+3)^2 * 2(x+y) $$ <br/>
위 식에 k값 다시 대입하면, <br/>
2. $$  3(k+3)^2 * 2(x+y) $$ <br/>
<span style="color:green;"> $$ = 6{({(x + y)}^2 + 3)}^2(x + y) $$  </span>
<br/>
<br/>

* ​왜 딥러닝에서 <b> 확률론 </b>이 필요한가?
1. 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있다
2. 회귀분석에서 손실함수로 사용되는 L2노름은 예측 오차의 분산을 `최소화`하는 방향으로 학습하도록 유도한다
3. 분류 문제에서 사용되는 교차엔트로피는 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도한다
4. 기계학습에서 사용되는 손실함수들의 작동원리는 데이터 공간을 통계적으로 해석해서 유도한다
<br/>
<br/>

* <b> Pytorch </b>
1. 자동미분 기능을 제공해, 쉽게 Gradient를 계산할 수 있다
2. CUDA를 통해, `GPU`를 이용해 고속 병렬 연산을 수행할 수 있다
3. 다양한 데이터 타입, 다양한 크기의 배열, 텐서를 지원한다
4. `동적` 연산 그래프를 사용해, 복잡한 모델의 구조를 간결하게 작성할 수 있다
5. 일차원 배열뿐만 아니라, 다양한 차원의 텐서를 지원하며, 딥러닝에 특화된 다양한 연산을 수행할 수 있다
<br/>
<br/>

* <b> 경사하강법을 이용한 최적화 기법 </b>
1. RMSProp(Root Mean Square Propagation)은 각 매개변수에 다른 학습률을 적용하여, 학습이 더 안정적이고 빠르게 수렴할 수 있도록 한다
2. Momentum은 기울기를 이용해 매개변수를 업데이트할 때, 이전 기울기를 고려해 관성을 만들어, 수렴 속도를 높이고 진동의 크기를 줄인다
3. Adagrad(Adaptive Gradient Algorithm)은 각 매개 변수에 대한 개별적 학습률을 조정하여, 자주 나타나는 특징에 대한 학습률을 `낮추고`, 드물게 나타나는 특징에 대한 학습률을 `높인다`
4. Adam(Adaptive Moment Estimation)은 모멘텀과 RMSProp 기법의 장점을 결합하여 학습률을 조정한다
5. 표준 경사 하강법은 전체 데이터 셋을 사용하는 방식이기 때문에 연산 비용이 크고, 메모리 사용량이 높은 편이다
<br/>
<br/>

* <b> CNN(Convolutional Neural Network) </b>
1. CNN은 주로 이미지 처리와 같은 2차원 or 3차원 구조를 가진 데이터 처리하는 데 사용되는 신경망 구조이다
2. 주요 구성요소에는 합성곱 층(Convolutional Layer), 풀링 층(Pooling Layer), 완전 연결 층(Fully Connected Layer)이 포함된다. 각각 특징추출, 차원축소, 최종출력계산 등의 역할을 한다
3. Convolutional Layer에서 사용되는 필터는 데이터의 공간적 패턴을 학습하는 데 도움을 준다
4. Pooling Layer는 입력되는 벡터의 차원을 줄이고, 과적합을 방지하는 역할을 한다
<br/>
<br/>

* <b> MLP </b>
: MLP의 여러 은닉층은 `복잡한 목제 해결 능력 향상`의 목적으로 사용된다
<br/>
<br/>

* <b> Sequential Model </b>
1. 복잡한 다중 입력, 다중 출력 모델을 구성할 수 `없다`
> 단순히 레이어를 쌓아가는 모델구조로 복잡한 입력/출력 모델을 구성하는 데는 제한이 있다
2. 딥러닝 프레임워크에서 제공하는 기본적 모델 구조 중 하나이다
3. 레이어를 선형적으로 쌓는 구조를 가진다
4. 레이어를 순차적으로 추가하여 모델을 구축한다
5. 주로 Keras와 같은 프레임워크에서 사용된다
<br/>
<br/>

> 네이버 boostcourse의 인공지능 기초 다지기 강의를 참조함