---
title: 2024.07.01
description: 경사하강법을 공부합니다.
categories: [TIL, Machine Learning]
tags: [til, machine learning, deep learning, gradient descent, sgd]
date: 2024-07-01 23:00:00 +0900
math: true
---

<hr>
* 미분 <br/>
: 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구 <br/>
함수 f의 주어진 점 (x, f(x)) 에서의 접선의 기울기를 구함 <br/>

$$ f(x) = x^2 + 2x + 3 $$ 

```python
import sympy as sym
from sympy.abc import x

sym.diff(sym.poly(x**2 + 2*x + 3), x)
```
{: .nolineno }

미분값을 빼면 경사하강법(gradient descent)
=> 함수값의 극소값의 위치를 구하고 극값에 도달하면 움직임을 멈춤

```python 
# 경사하강법 pseudo-code
var = init
grad = gradient(var)
while(abs(grad) > eps):
    var = var - lr * grad
    grad = gradient(var)
```
{: .nolineno }

변수가 벡터인 다변수 함수의 경우, 편미분(partial differentiation) 사용 <br/>

$$ f(x, y) = x^2 + 2xy + 3 + cos(x + 2y) $$

```python
# i번째 방향에서의 변화율만 계산
import sympy as sym
from sympy.abc import x, y

sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x)
```
{: .nolineno }

즉, 각 변수 별로 편미분을 계산한 그레디언트 벡터를 사용함 <br/>
-> 기차원 공간에서 벡터에 적용되는 경사하강법 사용 <br/>

![](/assets/img/gradient.png){: .shadow }

```python 
# 경사하강법 알고리즘 pseudo-code
# gradient: 그레디언트 벡터 계산 함수
# init: 시작점, lr: 학습률, eps: 알고리즘 종료조건
var = init
grad = gradient(var) 
while(norm(grad) > eps):
    var = var - lr * grad
    grad = gradient(var)
```
{: .nolineno }

무어-펜로즈 역행렬 이용 x => 경사하강법 이용하게 되면 선형모델이 아닌 다른 모델에도 이용 가능 <br/>

목적식을 최소화
즉, $$ L_2 norm = ||y - Xβ||_2 $$ 이를 최소화 하는 β 찾아야 함

* 경사하강법 기반 선형회귀 알고리즘 
: 역행렬을 이용하지 않고 회귀계수 계산 <br/>

```python
for t in range(T):
    error = y - X @ beta
    grad = - transpose(X) @ error
    beta = beta - lr * grad
    # 학습률 lr, 학습횟수 T가 중요한 하이퍼파라미터
```
{: .nolineno }

1. 선형회귀 
: 적절한 학습률과 학습횟수를 선택했을 때 수렴이 부장되어 있음 (미분가능, 회귀계수 β에 대해 convex한 함수이기 때문)
2. 비선형회귀
: 목적식이 nonconvex할 수 있기 때문에  수렴이 항상 보장 x
<hr>
* <span style="color:red;"> 확률적 경사하강법(stochastic gradient descent_SGD) </span>
: 모든 데이터x, 데이터 일부(Mini-Batch) 활용해 업데이트 <br/>
즉, 연산자원을 더 효율적으로 활용

볼록이 아닌(nonconvex) 목적식 => SGD를 통해 최적화(머신러닝 학습에 경사하강법보다 더 효율적) <br/>
따라서, 미니배치로 데이터를 쪼개면 병렬 연산이 가능해져 하드웨어 한계를 극복 가능할 수 있음 
<br/>  
<br/>

> 네이버 boostcourse의 인공지능 기초 다지기 강의를 참조함